{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm, trange\n",
    "import os, sys\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe():\n",
    "    def __init__(self, state=None):\n",
    "        self.reset()\n",
    "        if state is not None:\n",
    "            self.state = state\n",
    "        \n",
    "    def reset(self):\n",
    "        self.done = False\n",
    "        self.state = [0]*11\n",
    "        self.state[-1] = 1\n",
    "        return self.state\n",
    "    \n",
    "    class observation_space():\n",
    "        shape = (11,)\n",
    "\n",
    "    class action_space():\n",
    "        n = 9\n",
    "\n",
    "    def render(self):\n",
    "        # print whose turn it is when we render to look at what\n",
    "        # muzero thinks is a losing game\n",
    "        print(\"turn %d\" % self.state[-1])\n",
    "        print(np.array(self.state[0:9]).reshape(3,3))\n",
    "    \n",
    "    def value(self, s):\n",
    "        ret = 0\n",
    "        for turn in [-1, 1]:\n",
    "            for i in range(3):\n",
    "                if all([x==turn for x in s[3*i:3*i+3]]):\n",
    "                    ret = turn\n",
    "                if all([x==turn for x in [s[i], s[3+i], s[6+i]]]):\n",
    "                    ret = turn\n",
    "                if all([x==turn for x in [s[0], s[4], s[8]]]):\n",
    "                    ret = turn\n",
    "                if all([x==turn for x in [s[2], s[4], s[6]]]):\n",
    "                    ret = turn\n",
    "        # NOTE: this is not the value, the state may be won\n",
    "        return ret*s[-1]\n",
    "\n",
    "    def dynamics(self, s, act):\n",
    "        rew = 0\n",
    "        s = s.copy()\n",
    "        if s[act] != 0 or s[-2] != 0:\n",
    "          # don't move in taken spots or in finished games\n",
    "          rew = -10\n",
    "        else:\n",
    "            s[act] = s[-1]\n",
    "            rew += self.value(s)\n",
    "        if s[-2] != 0:\n",
    "            rew = 0\n",
    "        else:\n",
    "            s[-2] = self.value(s)\n",
    "        s[-1] = -s[-1]\n",
    "        return rew, s\n",
    "\n",
    "    def step(self, act):\n",
    "        rew, self.state = self.dynamics(self.state, act)\n",
    "        if rew != 0:\n",
    "            self.done = True\n",
    "        if np.all(np.array(self.state[0:9]) != 0):\n",
    "            self.done = True\n",
    "        return self.state, rew, self.done, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1], 0, False, None)\n",
      "([-1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], 0, False, None)\n",
      "([-1, 0, 0, 1, 1, 0, 0, 0, 0, 0, -1], 0, False, None)\n",
      "([-1, 0, -1, 1, 1, 0, 0, 0, 0, 0, 1], 0, False, None)\n",
      "([-1, 0, -1, 1, 1, 0, 1, 0, 0, 0, -1], 0, False, None)\n",
      "([-1, -1, -1, 1, 1, 0, 1, 0, 0, 1, 1], 1, True, None)\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToe()\n",
    "print(env.reset())\n",
    "print(env.step(4))\n",
    "print(env.step(0))\n",
    "print(env.step(3))\n",
    "print(env.step(2))\n",
    "print(env.step(6))\n",
    "print(env.step(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have mumodel below this is a mock model to better understand how the \n",
    "# MuModel works\n",
    "class MockModel():\n",
    "    def ht(self, s):\n",
    "        return s\n",
    "    def gt(self, s, a):\n",
    "        return env.dynamics(s, a)\n",
    "    def ft(s):\n",
    "        return [1/9]*9, env.value(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = [-1, -1, 0, 1, 1, 0, 1, 0, 0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turn 1\n",
      "[[-1 -1 -1]\n",
      " [ 1  1  0]\n",
      " [ 1  0  0]]\n",
      "turn 1\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "env.render()\n",
    "\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11,) 9\n"
     ]
    }
   ],
   "source": [
    "from muzero.model import MuModel\n",
    "m = MuModel(env.observation_space.shape, env.action_space.n, s_dim=64, K=5, lr=0.001)\n",
    "print(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "from muzero.game import Game, ReplayBuffer\n",
    "from muzero.mcts import naive_search, mcts_search\n",
    "replay_buffer = ReplayBuffer(50, 128, m.K)\n",
    "rews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, m):\n",
    "    import random\n",
    "    game = Game(env, discount=0.99)\n",
    "    while not game.terminal():\n",
    "    # TODO: Do we need to limit the depth of the MCTS search?\n",
    "    #policy = naive_search(m, game.observation, T=1)\n",
    "        policy, _ = mcts_search(m, game.observation, 30)\n",
    "        game.act_with_policy(policy)\n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1 [4, 6, 2, 1, 8, 3, 0] 120.75796508789062\n",
      "2 -10 [4, 4] 87.0246810913086\n",
      "2 -10 [4, 4] 41.70781326293945\n",
      "2 -10 [4, 4] 191.06112670898438\n",
      "3 -10 [4, 7, 7] 271.1387023925781\n",
      "2 -10 [4, 4] 25091.55078125\n",
      "3 -10 [4, 2, 4] 197.02207946777344\n",
      "5 1 [0, 7, 6, 4, 3] 203.1337127685547\n",
      "4 -10 [3, 7, 4, 4] 178.56204223632812\n",
      "5 1 [6, 3, 4, 0, 2] 161.0956573486328\n",
      "3 -10 [6, 4, 4] 158.32313537597656\n",
      "3 -10 [4, 7, 7] 123.99928283691406\n",
      "4 -10 [2, 7, 5, 7] 138.25648498535156\n",
      "3 -10 [0, 2, 0] 131.96624755859375\n",
      "3 -10 [4, 7, 7] 117.0551528930664\n"
     ]
    }
   ],
   "source": [
    "from muzero.model import reformat_batch\n",
    "import collections\n",
    "\n",
    "for j in range(30):\n",
    "    for i in range(10):\n",
    "        game = play_game(env, m)\n",
    "        replay_buffer.save_game(game)\n",
    "        rew = sum(game.rewards)\n",
    "        rews.append(rew)\n",
    "    for i in range(10):\n",
    "        m.train_on_batch(replay_buffer.sample_batch())\n",
    "    print(len(game.history), rew, game.history, m.losses[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(rews)\n",
    "figure()\n",
    "plt.yscale('log')\n",
    "plot([x[0] for x in m.losses])\n",
    "plot([x[1] for x in m.losses])\n",
    "plot([x[-3] for x in m.losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "@lru_cache(maxsize=none)\n",
    "\n",
    "def minimax(s):\n",
    "    # for each action, do the step method effectively\n",
    "    if env.value(s) != 0:\n",
    "        return s[-1] * env.value(s)\n",
    "    \n",
    "    print(s)\n",
    "    if s[-1] == 1:\n",
    "        value = -float('inf')\n",
    "        for a in range(9):\n",
    "            rew, ns = env.dynamics(s, a)\n",
    "            value = max(value, minimax(ns))\n",
    "        if s[-1] == -1:\n",
    "            value = float('inf')\n",
    "            for a in range(9):\n",
    "            rew, ns = env.dynamics(s, a)\n",
    "            if s != ns:\n",
    "                value = min(value, minimax(ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "minimax(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
